{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQbZ/k+Wja7u4nrbUDkZAm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FelixMaturano/SIS420/blob/main/brazo_Robotico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. Entorno del Brazo Robótico (ArmEnv)"
      ],
      "metadata": {
        "id": "csVVLk55Yah9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium pybullet torch numpy opencv-python"
      ],
      "metadata": {
        "id": "nxqh53IQZlu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIj8HB8cX1J3"
      },
      "outputs": [],
      "source": [
        "import os, math, pybullet as p, pybullet_data, gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "URDF_ROOT = Path(pybullet_data.getDataPath())\n",
        "\n",
        "class ArmEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 60}\n",
        "\n",
        "    def __init__(self, render_mode: str | None = None, max_steps: int = 300):\n",
        "        super().__init__()\n",
        "        self.render_mode = render_mode\n",
        "        self.client = p.connect(p.GUI if render_mode == \"human\" else p.DIRECT)\n",
        "        p.setAdditionalSearchPath(str(URDF_ROOT))\n",
        "        p.setGravity(0, 0, -9.81, physicsClientId=self.client)\n",
        "\n",
        "        # Cargar mesa, brazo y vaso\n",
        "        self.plane = p.loadURDF(\"plane.urdf\", physicsClientId=self.client)\n",
        "        self.table = p.loadURDF(\"table/table.urdf\", [0.5, 0, 0], p.getQuaternionFromEuler([0, 0, 0]), physicsClientId=self.client)\n",
        "        self.arm = p.loadURDF(\"kuka_iiwa/model.urdf\", [0, 0, 0.7], useFixedBase=True, physicsClientId=self.client)\n",
        "        self.cup_visual = p.createVisualShape(p.GEOM_CYLINDER, radius=0.035, length=0.1, rgbaColor=[0.95, 0.9, 0.8, 1])\n",
        "        self.cup_collision = p.createCollisionShape(p.GEOM_CYLINDER, radius=0.035, height=0.1)\n",
        "        self.cup = None\n",
        "\n",
        "        self.max_steps = max_steps\n",
        "        self.step_ctr = 0\n",
        "        self._setup_spaces()\n",
        "\n",
        "    def reset(self, seed: int | None = None, options: dict | None = None):\n",
        "        super().reset(seed=seed)\n",
        "        self.step_ctr = 0\n",
        "        if self.cup is not None:\n",
        "            p.removeBody(self.cup, physicsClientId=self.client)\n",
        "\n",
        "        # Posición aleatoria del vaso\n",
        "        xy = self.np_random.uniform(low=-0.5, high=0.5, size=(2,))\n",
        "        cup_pos = [xy[0] + 0.5, xy[1], 0.82]  # sobre la mesa\n",
        "        self.cup = p.createMultiBody(baseMass=0.05,\n",
        "                                     baseCollisionShapeIndex=self.cup_collision,\n",
        "                                     baseVisualShapeIndex=self.cup_visual,\n",
        "                                     basePosition=cup_pos,\n",
        "                                     baseOrientation=[0, 0, 0, 1],\n",
        "                                     physicsClientId=self.client)\n",
        "\n",
        "        # Configuración home del brazo\n",
        "        home_q = [0, -1.0, 0, -1.7, 0, 1.2, 0]\n",
        "        for j, q in enumerate(home_q):\n",
        "            p.resetJointState(self.arm, j, q, physicsClientId=self.client)\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        info = {}\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action: np.ndarray):\n",
        "        self.step_ctr += 1\n",
        "        # Control de juntas\n",
        "        dv = 0.05  # rad/step\n",
        "        for j in range(7):\n",
        "            cur_q = p.getJointState(self.arm, j, physicsClientId=self.client)[0]\n",
        "            target_q = cur_q + np.clip(action[j], -1, 1) * dv\n",
        "            p.setJointMotorControl2(self.arm, j, p.POSITION_CONTROL, targetPosition=target_q, force=200, physicsClientId=self.client)\n",
        "\n",
        "        # Empuje vertical\n",
        "        push_strength = np.clip(action[-1], 0, 1)\n",
        "        p.applyExternalForce(objectUniqueId=self.arm, linkIndex=6,\n",
        "                             forceObj=[0, 0, -50 * push_strength],\n",
        "                             posObj=[0, 0, 0], flags=p.LINK_FRAME,\n",
        "                             physicsClientId=self.client)\n",
        "\n",
        "        p.stepSimulation(physicsClientId=self.client)\n",
        "        if self.render_mode == \"human\":\n",
        "            p.configureDebugVisualizer(p.COV_ENABLE_SINGLE_STEP_RENDERING, 1, physicsClientId=self.client)\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        reward, terminated, truncated = self._get_reward_done()\n",
        "        info = {}\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode == \"rgb_array\":\n",
        "            width, height = 640, 480\n",
        "            view_matrix = p.computeViewMatrixFromYawPitchRoll(\n",
        "                cameraTargetPosition=[0.5, 0, 0.5],\n",
        "                distance=1.5,\n",
        "                yaw=45,\n",
        "                pitch=-30,\n",
        "                roll=0,\n",
        "                upAxisIndex=2,\n",
        "                physicsClientId=self.client)\n",
        "            proj_matrix = p.computeProjectionMatrixFOV(\n",
        "                fov=60, aspect=width/height, nearVal=0.1, farVal=10.0)\n",
        "            img = p.getCameraImage(width, height, view_matrix, proj_matrix,\n",
        "                                 renderer=p.ER_BULLET_HARDWARE_OPENGL,\n",
        "                                 physicsClientId=self.client)[2]\n",
        "            return np.asarray(img)\n",
        "        return None\n",
        "\n",
        "    def close(self):\n",
        "        p.disconnect(self.client)\n",
        "\n",
        "    def _setup_spaces(self):\n",
        "        high = np.array([1] * 10, dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
        "        self.action_space = spaces.Box(low=-1, high=1, shape=(8,), dtype=np.float32)\n",
        "\n",
        "    def _get_obs(self):\n",
        "        cup_pos, _ = p.getBasePositionAndOrientation(self.cup, physicsClientId=self.client)\n",
        "        cup_pos = np.array(cup_pos, dtype=np.float32)\n",
        "        q = [p.getJointState(self.arm, j, physicsClientId=self.client)[0] for j in range(7)]\n",
        "        q = np.array(q, dtype=np.float32)\n",
        "        obs = np.concatenate([cup_pos, q])\n",
        "        # Normalización\n",
        "        obs[:3] = obs[:3] - np.array([0.5, 0, 0.82])\n",
        "        obs[3:] = q / math.pi\n",
        "        return obs\n",
        "\n",
        "    def _get_reward_done(self):\n",
        "        cup_pos, cup_orient = p.getBasePositionAndOrientation(self.cup, physicsClientId=self.client)\n",
        "        z_up = p.getMatrixFromQuaternion(cup_orient)[2]\n",
        "        done_success = z_up < 0\n",
        "        cup_height_ok = cup_pos[2] > 0.79\n",
        "\n",
        "        reward = 0.0\n",
        "        if done_success and cup_height_ok:\n",
        "            reward += 1.0\n",
        "        elif cup_height_ok:\n",
        "            reward += 0.5 * (1 - abs(z_up))\n",
        "        else:\n",
        "            reward -= 1.0\n",
        "        reward -= 1.0 / self.max_steps\n",
        "\n",
        "        terminated = done_success or not cup_height_ok\n",
        "        truncated = self.step_ctr >= self.max_steps\n",
        "        return reward, terminated, truncated"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, img_channels: int, state_dim: int, action_dim: int):\n",
        "        super().__init__()\n",
        "        # CNN para imágenes (placeholder)\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(img_channels, 32, 8, stride=4), nn.GELU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2), nn.GELU(),\n",
        "            nn.Conv2d(64, 64, 3, stride=1), nn.GELU(),\n",
        "            nn.Flatten())\n",
        "        cnn_out = 7 * 7 * 64  # para 84×84\n",
        "\n",
        "        # MLP para estados\n",
        "        self.mlp_state = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128), nn.GELU(),\n",
        "            nn.Linear(128, 128), nn.GELU())\n",
        "\n",
        "        # Fusión\n",
        "        fusion_dim = cnn_out + 128\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(fusion_dim, 256), nn.GELU())\n",
        "\n",
        "        # Cabezas de política y valor\n",
        "        self.policy_mu = nn.Linear(256, action_dim)\n",
        "        self.policy_logstd = nn.Parameter(torch.zeros(action_dim))\n",
        "        self.value_head = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, img: torch.Tensor, state: torch.Tensor):\n",
        "        img_feat = self.cnn(img / 255.0)\n",
        "        state_feat = self.mlp_state(state)\n",
        "        x = torch.cat([img_feat, state_feat], dim=-1)\n",
        "        x = self.fusion(x)\n",
        "        return self.policy_mu(x), self.policy_logstd.exp(), self.value_head(x).squeeze(-1)\n",
        "\n",
        "    def act(self, img, state):\n",
        "        mu, std, _ = self.forward(img, state)\n",
        "        dist = torch.distributions.Normal(mu, std)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action).sum(-1)\n",
        "        return action, log_prob, dist\n",
        "\n",
        "    def value(self, img, state):\n",
        "        _, _, v = self.forward(img, state)\n",
        "        return v\n"
      ],
      "metadata": {
        "id": "JL---dYkYsPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RolloutBuffer:\n",
        "    def __init__(self, size, obs_shape, state_dim, action_dim, device):\n",
        "        self.size = size\n",
        "        self.ptr = 0\n",
        "        self.full = False\n",
        "        self.device = device\n",
        "        self.imgs = torch.zeros((size, *obs_shape), dtype=torch.uint8, device=device)\n",
        "        self.states = torch.zeros((size, state_dim), device=device)\n",
        "        self.actions = torch.zeros((size, action_dim), device=device)\n",
        "        self.rewards = torch.zeros(size, device=device)\n",
        "        self.dones = torch.zeros(size, device=device)\n",
        "        self.logprobs = torch.zeros((size, action_dim), device=device)\n",
        "        self.values = torch.zeros(size, device=device)\n",
        "        self.advantages = torch.zeros(size, device=device)\n",
        "        self.returns = torch.zeros(size, device=device)\n",
        "\n",
        "    def add(self, img, state, action, reward, done, logprob, value):\n",
        "        self.imgs[self.ptr] = img\n",
        "        self.states[self.ptr] = state\n",
        "        self.actions[self.ptr] = action\n",
        "        self.rewards[self.ptr] = reward\n",
        "        self.dones[self.ptr] = done\n",
        "        self.logprobs[self.ptr] = logprob\n",
        "        self.values[self.ptr] = value\n",
        "        self.ptr += 1\n",
        "        if self.ptr >= self.size:\n",
        "            self.full = True\n",
        "            self.ptr = 0\n",
        "\n",
        "    def compute_returns_advantages(self, last_value, gamma=0.99, lam=0.95):\n",
        "        gae = 0\n",
        "        for i in reversed(range(self.size)):\n",
        "            delta = self.rewards[i] + gamma * (1 - self.dones[i]) * (\n",
        "                last_value if i == self.size - 1 else self.values[i + 1]) - self.values[i]\n",
        "            gae = delta + gamma * lam * (1 - self.dones[i]) * gae\n",
        "            self.advantages[i] = gae\n",
        "            self.returns[i] = gae + self.values[i]"
      ],
      "metadata": {
        "id": "yJX7y07ZYwRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPO:\n",
        "    def __init__(self, actor_critic: nn.Module, lr=3e-4, eps_clip=0.2,\n",
        "                 vf_coef=0.5, ent_coef=0.01, max_grad_norm=0.5, device=\"cpu\"):\n",
        "        self.ac = actor_critic\n",
        "        self.optimizer = Adam(self.ac.parameters(), lr=lr)\n",
        "        self.eps_clip = eps_clip\n",
        "        self.vf_coef = vf_coef\n",
        "        self.ent_coef = ent_coef\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.device = device\n",
        "\n",
        "    def update(self, buffer: RolloutBuffer, batch_size=64, epochs=10):\n",
        "        idxs = torch.arange(buffer.size, device=self.device)\n",
        "        adv = (buffer.advantages - buffer.advantages.mean()) / (buffer.advantages.std() + 1e-8)\n",
        "\n",
        "        for _ in range(epochs):\n",
        "            perm = idxs[torch.randperm(buffer.size)]\n",
        "            for i in range(0, buffer.size, batch_size):\n",
        "                batch = perm[i:i+batch_size]\n",
        "                imgs = buffer.imgs[batch].float()\n",
        "                states = buffer.states[batch]\n",
        "                actions = buffer.actions[batch]\n",
        "                old_logp = buffer.logprobs[batch]\n",
        "                returns = buffer.returns[batch]\n",
        "                adv_b = adv[batch]\n",
        "\n",
        "                mu, std, values = self.ac(imgs, states)\n",
        "                dist = torch.distributions.Normal(mu, std)\n",
        "                logp = dist.log_prob(actions).sum(-1)\n",
        "                ratio = torch.exp(logp - old_logp.sum(-1))\n",
        "\n",
        "                surr1 = ratio * adv_b\n",
        "                surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * adv_b\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "                value_loss = F.mse_loss(values, returns)\n",
        "                entropy = dist.entropy().sum(-1).mean()\n",
        "\n",
        "                loss = policy_loss + self.vf_coef * value_loss - self.ent_coef * entropy\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.ac.parameters(), self.max_grad_norm)\n",
        "                self.optimizer.step()"
      ],
      "metadata": {
        "id": "9kmRk80RY1jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.vector import AsyncVectorEnv\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Configuración\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "N_ENVS = 4  # Reducido para Colab\n",
        "ROLLOUT_STEPS = 512  # Reducido para Colab\n",
        "TOTAL_UPDATES = 1000  # Reducido para demostración\n",
        "\n",
        "# Crear entorno\n",
        "def make_env(render=False):\n",
        "    def _thunk():\n",
        "        return ArmEnv(render_mode=\"human\" if render else None)\n",
        "    return _thunk\n",
        "\n",
        "vec_env = AsyncVectorEnv([make_env(render=(i==0)) for i in range(N_ENVS)])\n",
        "obs, _ = vec_env.reset()\n",
        "\n",
        "# Dimensiones (usando imagen dummy)\n",
        "img_shape = (1, 84, 84)  # 1 canal (escala de grises)\n",
        "state_dim = obs.shape[1]  # 10\n",
        "\n",
        "# Modelo y algoritmo\n",
        "ac = ActorCritic(img_channels=1, state_dim=state_dim, action_dim=8).to(DEVICE)\n",
        "ppo = PPO(ac, device=DEVICE)\n",
        "buffer = RolloutBuffer(ROLLOUT_STEPS * N_ENVS, img_shape, state_dim, 8, DEVICE)\n",
        "\n",
        "# Bucle de entrenamiento\n",
        "for update in range(TOTAL_UPDATES):\n",
        "    for step in range(ROLLOUT_STEPS):\n",
        "        # Observaciones (imagen dummy por ahora)\n",
        "        img = torch.zeros((N_ENVS, *img_shape), dtype=torch.uint8, device=DEVICE)\n",
        "        state = torch.tensor(obs, dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action, logp, _ = ac.act(img, state)\n",
        "            value = ac.value(img, state)\n",
        "\n",
        "        actions_np = action.cpu().numpy()\n",
        "        next_obs, rewards, terms, truncs, _ = vec_env.step(actions_np)\n",
        "\n",
        "        # Almacenar en buffer\n",
        "        for e in range(N_ENVS):\n",
        "            buffer.add(img[e], state[e], action[e], rewards[e],\n",
        "                      terms[e] or truncs[e], logp[e], value[e])\n",
        "        obs = next_obs\n",
        "\n",
        "    # Calcular ventajas\n",
        "    with torch.no_grad():\n",
        "        last_img = torch.zeros((N_ENVS, *img_shape), dtype=torch.uint8, device=DEVICE)\n",
        "        last_state = torch.tensor(obs, dtype=torch.float32, device=DEVICE)\n",
        "        last_value = ac.value(last_img, last_state).cpu()\n",
        "\n",
        "    buffer.compute_returns_advantages(last_value)\n",
        "\n",
        "    # Actualizar política\n",
        "    ppo.update(buffer, batch_size=64, epochs=4)\n",
        "\n",
        "    print(f\"Update {update+1}/{TOTAL_UPDATES} - Avg Reward: {buffer.rewards.mean():.2f}\")\n",
        "\n",
        "# Cerrar entorno al finalizar\n",
        "vec_env.close()"
      ],
      "metadata": {
        "id": "-S8Q2C24Y5TC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}